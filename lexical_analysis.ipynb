{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducing th elexical analysis done in Aguirre ert al using log odd rario informative direlicht prior\n",
    "- use LIWC tcategroy count instead of using vocab count\n",
    "- use log of frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>[name]</th>\n",
       "      <th>[race]</th>\n",
       "      <th>[psych]</th>\n",
       "      <th>[gender]</th>\n",
       "      <th>[text]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Chloe</td>\n",
       "      <td>a white</td>\n",
       "      <td>PTSD</td>\n",
       "      <td>f</td>\n",
       "      <td>I am tired of the lack of diversity in the te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Chloe</td>\n",
       "      <td>a white</td>\n",
       "      <td>depression</td>\n",
       "      <td>f</td>\n",
       "      <td>I have a lot of privilege. I'm well-educated,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Chloe</td>\n",
       "      <td>a black</td>\n",
       "      <td>PTSD</td>\n",
       "      <td>f</td>\n",
       "      <td>I am constantly feeling like I'm not good eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Chloe</td>\n",
       "      <td>a black</td>\n",
       "      <td>depression</td>\n",
       "      <td>f</td>\n",
       "      <td>I often feel like I have to be twice as good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Chloe</td>\n",
       "      <td>an asian</td>\n",
       "      <td>PTSD</td>\n",
       "      <td>f</td>\n",
       "      <td>I have felt the immense pressure to succeed i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 [name]    [race]     [psych] [gender]  \\\n",
       "0           0  Chloe   a white        PTSD        f   \n",
       "1           1  Chloe   a white  depression        f   \n",
       "2           2  Chloe   a black        PTSD        f   \n",
       "3           3  Chloe   a black  depression        f   \n",
       "4           4  Chloe  an asian        PTSD        f   \n",
       "\n",
       "                                              [text]  \n",
       "0   I am tired of the lack of diversity in the te...  \n",
       "1   I have a lot of privilege. I'm well-educated,...  \n",
       "2   I am constantly feeling like I'm not good eno...  \n",
       "3   I often feel like I have to be twice as good ...  \n",
       "4   I have felt the immense pressure to succeed i...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/home/shinkamo/598sdl/data/synthetic/1.csv')\n",
    "df2 = pd.read_csv('/home/shinkamo/598sdl/data/synthetic/2.csv')\n",
    "df3 = pd.read_csv('/home/shinkamo/598sdl/data/synthetic/3.csv')\n",
    "df = pd.concat([df1, df2, df3])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "def clean_text(x):\n",
    "    temp_text = x.lower()\n",
    "    # Remove punctuation of the string\n",
    "    temp_text = temp_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove new line from string\n",
    "    temp_text = temp_text.replace('\\n', '')\n",
    "    # Remove double space or more\n",
    "    temp_text = re.sub(' +', ' ', temp_text).strip()\n",
    "    # Tokenized the text\n",
    "    temp_text = nltk.word_tokenize(temp_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_word = []\n",
    "    \n",
    "    for word in temp_text:\n",
    "        # Lemmanize and stem word\n",
    "        lemma_word = wordnet_lemmatizer.lemmatize(word)\n",
    "        stemmed_word = englishStemmer.stem(lemma_word)\n",
    "        \n",
    "        # Do not add stop words into the the final cleaned sentence\n",
    "        if stemmed_word in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            filtered_word.append(stemmed_word)\n",
    "            \n",
    "    return \" \".join(filtered_word).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_liwc():\n",
    "    \"\"\" Load LIWC data \"\"\"\n",
    "\n",
    "    with open(\"/home/public/LinguisticEthnography/LIWC/LIWC.2015.all.txt\", \"r\") as file_p:\n",
    "        data = file_p.readlines()\n",
    "    liwc = defaultdict(list)\n",
    "    for entry in data:\n",
    "        entry = entry.split(' ,')\n",
    "        liwc[entry[1].rstrip('\\n')].append(entry[0])\n",
    "    return liwc\n",
    "\n",
    "class LIWC:\n",
    "    def __init__(self):\n",
    "        self.liwc = load_liwc()\n",
    "\n",
    "    def count_liwc(self, sentence, categories=None):\n",
    "        \"\"\"\n",
    "        Count liwc categories\n",
    "        \"\"\"\n",
    "        if not categories:\n",
    "            categories = []\n",
    "\n",
    "        sentence = clean_text(sentence)\n",
    "        counts = {}\n",
    "        for category, tokens in self.liwc.items():\n",
    "            counts[category] = sum([sentence.count(_tok) for _tok in tokens])\n",
    "        return counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting LI count per category(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem = df[df['[gender]'] == 'f']\n",
    "fem_text = \"\"\n",
    "for t in fem['[text]']:\n",
    "    fem_text += t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc = LIWC()\n",
    "f_liwc = liwc.count_liwc(fem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "masc = df[df['[gender]'] == 'm']\n",
    "masc_text = \"\"\n",
    "for t in masc['[text]']:\n",
    "    masc_text += t\n",
    "m_liwc = liwc.count_liwc(masc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "- y - wc for topic\n",
    "- a = prior, estimated by avg count pe category\n",
    "\n",
    "groups = female, male, i\n",
    "topic = liwc, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc = LIWC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"i'd\",\n",
       " \"i'd've\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'id',\n",
       " 'idc',\n",
       " 'idgaf',\n",
       " 'idk',\n",
       " 'idontknow',\n",
       " 'idve',\n",
       " 'ikr',\n",
       " 'ily*',\n",
       " 'im',\n",
       " 'ima',\n",
       " 'imean',\n",
       " 'imma',\n",
       " 'ive',\n",
       " 'me',\n",
       " 'methinks',\n",
       " 'mine',\n",
       " 'my',\n",
       " 'myself']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc.liwc['I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = ['MONEY', 'REWARD']\n",
    "relationships = ['FAMILY', 'HOME']\n",
    "selves = ['I']\n",
    "social = ['SHEHE', 'YOU', 'WE', 'THEY']\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.mean(list(f_liwc.values()))\n",
    "n  = np.sum(list(f_liwc.values())) + np.sum(list(m_liwc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dict = defaultdict(list,{ k:[] for k in liwc.liwc.keys() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = health\n",
    "n_ik = np.sum([f_liwc[i] for i in cat])\n",
    "n_k = np.sum([m_liwc[i] for i in cat]) + np.sum([m_liwc[i] for i in cat])\n",
    "\n",
    "\n",
    "for c in cat:\n",
    "    y_ikw = f_liwc[c]\n",
    "    y_kw = f_liwc[c] + m_liwc[c]\n",
    "    a_ikw = (alpha/n)*y_ikw\n",
    "    a_kw = (alpha/n)*y_kw\n",
    "\n",
    "    log_odd = np.log((y_ikw + a_ikw)/(n + alpha- y_kw - a_ikw)) - np.log((y_kw  + a_kw)/(n_k+ alpha - y_kw- a_kw))\n",
    "    f_dict[c] = log_odd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_odd(cat, f_dict, i_liwc, j_liwc):\n",
    "    n_ik = np.sum([i_liwc[i] for i in cat])\n",
    "    n_k = np.sum([j_liwc[i] for i in cat]) + np.sum([j_liwc[i] for i in cat])\n",
    "\n",
    "\n",
    "    for c in cat:\n",
    "        y_ikw = i_liwc[c]\n",
    "        y_kw = i_liwc[c] + j_liwc[c]\n",
    "        a_ikw = (alpha/n)*y_ikw\n",
    "        a_kw = (alpha/n)*y_kw\n",
    "\n",
    "        log_odd = np.log((y_ikw + a_ikw)/(n_ik + alpha- y_kw - a_ikw)) - np.log((y_kw  + a_kw)/(n_k+ alpha - y_kw- a_kw))\n",
    "        f_dict[c] = log_odd\n",
    "\n",
    "    return f_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dict = defaultdict(list,{ k:[] for k in liwc.liwc.keys() })\n",
    "\n",
    "lo_health = calculate_log_odd(health, f_dict, f_liwc, m_liwc)\n",
    "lo_rel = calculate_log_odd(relationships, lo_health,f_liwc,m_liwc)\n",
    "lo_selves = calculate_log_odd(relationships, lo_rel,f_liwc,  m_liwc)\n",
    "f_lo = calculate_log_odd(relationships, lo_selves,f_liwc, m_liwc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'RELIG': [],\n",
       "             'BODY': [],\n",
       "             'COMPARE': [],\n",
       "             'MONEY': -0.34171488931504923,\n",
       "             'PRONOUN': [],\n",
       "             'FEEL': [],\n",
       "             'CERTAIN': [],\n",
       "             'INSIGHT': [],\n",
       "             'ASSENT': [],\n",
       "             'NUMBER': [],\n",
       "             'NEGATE': [],\n",
       "             'SAD': [],\n",
       "             'AFFILIATION': [],\n",
       "             'COGPROC': [],\n",
       "             'FEMALE': [],\n",
       "             'AFFECT': [],\n",
       "             'ANGER': [],\n",
       "             'HOME': -0.3952751313103329,\n",
       "             'CONJ': [],\n",
       "             'SEXUAL': [],\n",
       "             'NEGEMO': [],\n",
       "             'PPRON': [],\n",
       "             'DIFFER': [],\n",
       "             'DEATH': [],\n",
       "             'FAMILY': -0.5015726742987066,\n",
       "             'ADVERB': [],\n",
       "             'SPACE': [],\n",
       "             'INFORMAL': [],\n",
       "             'POSEMO': [],\n",
       "             'ANX': [],\n",
       "             'FOCUSPRESENT': [],\n",
       "             'NETSPEAK': [],\n",
       "             'PERCEPT': [],\n",
       "             'HEALTH': [],\n",
       "             'DISCREP': [],\n",
       "             'RELATIV': [],\n",
       "             'QUANT': [],\n",
       "             'NONFLU': [],\n",
       "             'ADJ': [],\n",
       "             'PREP': [],\n",
       "             'FRIEND': [],\n",
       "             'FUNCTION': [],\n",
       "             'BIO': [],\n",
       "             'TENTAT': [],\n",
       "             'RISK': [],\n",
       "             'POWER': [],\n",
       "             'INTERROG': [],\n",
       "             'SOCIAL': [],\n",
       "             'DRIVES': [],\n",
       "             'LEISURE': [],\n",
       "             'WE': [],\n",
       "             'VERB': [],\n",
       "             'HEAR': [],\n",
       "             'FOCUSPAST': [],\n",
       "             'THEY': [],\n",
       "             'ARTICLE': [],\n",
       "             'YOU': [],\n",
       "             'MALE': [],\n",
       "             'IPRON': [],\n",
       "             'I': [],\n",
       "             'CAUSE': [],\n",
       "             'WORK': [],\n",
       "             'SEE': [],\n",
       "             'INGEST': [],\n",
       "             'MOTION': [],\n",
       "             'FILLER': [],\n",
       "             'ACHIEV': [],\n",
       "             'SWEAR': [],\n",
       "             'FOCUSFUTURE': [],\n",
       "             'TIME': [],\n",
       "             'AUXVERB': [],\n",
       "             'REWARD': -0.3038788140634252,\n",
       "             'SHEHE': []})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lo_social\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict = defaultdict(list,{ k:[] for k in liwc.liwc.keys() })\n",
    "\n",
    "lo_health = calculate_log_odd(health, m_dict,  m_liwc, f_liwc)\n",
    "lo_rel = calculate_log_odd(relationships, lo_health,m_liwc,f_liwc)\n",
    "lo_selves = calculate_log_odd(relationships, lo_rel,m_liwc, f_liwc)\n",
    "m_lo = calculate_log_odd(relationships, lo_selves,m_liwc, f_liwc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'RELIG': [],\n",
       "             'BODY': [],\n",
       "             'COMPARE': [],\n",
       "             'MONEY': -0.34171488931504923,\n",
       "             'PRONOUN': [],\n",
       "             'FEEL': [],\n",
       "             'CERTAIN': [],\n",
       "             'INSIGHT': [],\n",
       "             'ASSENT': [],\n",
       "             'NUMBER': [],\n",
       "             'NEGATE': [],\n",
       "             'SAD': [],\n",
       "             'AFFILIATION': [],\n",
       "             'COGPROC': [],\n",
       "             'FEMALE': [],\n",
       "             'AFFECT': [],\n",
       "             'ANGER': [],\n",
       "             'HOME': -0.3952751313103329,\n",
       "             'CONJ': [],\n",
       "             'SEXUAL': [],\n",
       "             'NEGEMO': [],\n",
       "             'PPRON': [],\n",
       "             'DIFFER': [],\n",
       "             'DEATH': [],\n",
       "             'FAMILY': -0.5015726742987066,\n",
       "             'ADVERB': [],\n",
       "             'SPACE': [],\n",
       "             'INFORMAL': [],\n",
       "             'POSEMO': [],\n",
       "             'ANX': [],\n",
       "             'FOCUSPRESENT': [],\n",
       "             'NETSPEAK': [],\n",
       "             'PERCEPT': [],\n",
       "             'HEALTH': [],\n",
       "             'DISCREP': [],\n",
       "             'RELATIV': [],\n",
       "             'QUANT': [],\n",
       "             'NONFLU': [],\n",
       "             'ADJ': [],\n",
       "             'PREP': [],\n",
       "             'FRIEND': [],\n",
       "             'FUNCTION': [],\n",
       "             'BIO': [],\n",
       "             'TENTAT': [],\n",
       "             'RISK': [],\n",
       "             'POWER': [],\n",
       "             'INTERROG': [],\n",
       "             'SOCIAL': [],\n",
       "             'DRIVES': [],\n",
       "             'LEISURE': [],\n",
       "             'WE': [],\n",
       "             'VERB': [],\n",
       "             'HEAR': [],\n",
       "             'FOCUSPAST': [],\n",
       "             'THEY': [],\n",
       "             'ARTICLE': [],\n",
       "             'YOU': [],\n",
       "             'MALE': [],\n",
       "             'IPRON': [],\n",
       "             'I': [],\n",
       "             'CAUSE': [],\n",
       "             'WORK': [],\n",
       "             'SEE': [],\n",
       "             'INGEST': [],\n",
       "             'MOTION': [],\n",
       "             'FILLER': [],\n",
       "             'ACHIEV': [],\n",
       "             'SWEAR': [],\n",
       "             'FOCUSFUTURE': [],\n",
       "             'TIME': [],\n",
       "             'AUXVERB': [],\n",
       "             'REWARD': -0.3038788140634252,\n",
       "             'SHEHE': []})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lo_social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31a04c3eb7f40af14b79fdd3ee347173dfaaafd6f4e6f37383333d775633736"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
